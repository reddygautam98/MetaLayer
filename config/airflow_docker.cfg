# ================================================================
# AIRFLOW CONFIGURATION FOR DOCKER METALAYER ETL PIPELINE
# ================================================================
# 
# This configuration file sets up Airflow for the MetaLayer ETL pipeline
# with Docker-optimized settings for production deployment.
#
# Features:
# - CeleryExecutor for distributed processing
# - PostgreSQL backend configuration
# - Redis message broker setup
# - Security and authentication settings
# - Performance optimization
# - Monitoring and logging configuration
#
# Usage: Place this file as airflow.cfg in your Airflow configuration directory
# or set AIRFLOW__CORE__CONFIG_FILE environment variable to point to this file
# ================================================================

[core]
# Core configuration
dags_folder = /opt/airflow/dags
hostname_callable = airflow.utils.net.getfqdn
default_timezone = UTC
executor = CeleryExecutor
parallelism = 32
dag_concurrency = 16
dags_are_paused_at_creation = False
max_active_runs_per_dag = 1
load_examples = False
plugins_folder = /opt/airflow/plugins
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow_password@postgres:5432/airflow
fernet_key = ZmDfcTF7_60GrrY167zsiPd67pEvs0aGOv2oasOM1Pg=

# DAG Processing
dag_file_processor_timeout = 50
dagbag_import_timeout = 30
dag_discovery_safe_mode = True
store_dag_code = True
store_serialized_dags = True
min_serialized_dag_update_interval = 30
max_num_rendered_ti_fields_per_task = 30

# Task execution
default_task_retries = 1
task_runner = StandardTaskRunner
killed_task_cleanup_time = 60

# Security
secure_mode = False
donot_pickle = True
enable_xcom_pickling = False

[logging]
# Logging configuration
base_log_folder = /opt/airflow/logs
remote_logging = False
remote_log_conn_id = 
remote_base_log_folder = 
log_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log
log_processor_filename_template = {{ filename }}.log
dag_processor_manager_log_location = /opt/airflow/logs/dag_processor_manager/dag_processor_manager.log
colored_console_log = True
colored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s
colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s
fab_logging_level = WARNING

[celery]
# Celery configuration for distributed processing
celery_app_name = airflow.executors.celery_executor
worker_concurrency = 16
worker_log_server_port = 8793
broker_url = redis://:redis_password@redis:6379/0
result_backend = db+postgresql://airflow:airflow_password@postgres:5432/airflow
flower_host = 0.0.0.0
flower_port = 5555
flower_url_prefix = 
default_queue = default
celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG
sync_parallelism = 0
worker_enable_remote_control = true

[celery_kubernetes_executor]
kubernetes_queue = kubernetes

[database]
# Database configuration
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow_password@postgres:5432/airflow
sql_engine_encoding = utf-8
sql_alchemy_pool_enabled = True
sql_alchemy_pool_size = 5
sql_alchemy_max_overflow = 10
sql_alchemy_pool_recycle = 1800
sql_alchemy_pool_pre_ping = True
sql_alchemy_schema = 
max_db_retries = 3

[operators]
# Operator configuration
default_owner = metalayer-team
default_cpus = 1
default_ram = 512
default_disk = 512
default_gpus = 0
allow_illegal_arguments = False

[webserver]
# Web server configuration
base_url = http://localhost:8080
default_ui_timezone = UTC
web_server_host = 0.0.0.0
web_server_port = 8080
web_server_ssl_cert = 
web_server_ssl_key = 
web_server_worker_timeout = 120
worker_refresh_batch_size = 1
worker_refresh_interval = 30
reload_on_plugin_change = False
secret_key = temporary_key
workers = 4
worker_class = sync
access_logfile = -
error_logfile = -
access_logformat = 
expose_config = True
expose_hostname = True
expose_stacktrace = True
dag_default_view = tree
dag_orientation = LR
log_fetch_timeout_sec = 5
log_fetch_delay_sec = 2
log_auto_tailing_offset = 30
log_animation_speed = 1000
hide_paused_dags_by_default = False
page_size = 100
navbar_color = 
default_dag_run_display_number = 25
enable_proxy_fix = False
cookie_secure = False
cookie_samesite = Lax
default_wrap = False
x_frame_enabled = True
show_recent_stats_for_completed_runs = True
update_fab_perms = True

[email]
# Email configuration
email_backend = airflow.utils.email.send_email_smtp
email_conn_id = smtp_default
default_email_on_retry = True
default_email_on_failure = True

[smtp]
# SMTP configuration for email alerts
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_port = 587
smtp_mail_from = airflow@metalayer.com
smtp_timeout = 30
smtp_retry_limit = 5

[sentry]
# Sentry integration (optional)
sentry_dsn = 
sentry_on = False

[local_kubernetes_executor]
kubernetes_queue = kubernetes

[kubernetes]
# Kubernetes configuration (if using KubernetesExecutor)
namespace = airflow
airflow_configmap = 
worker_container_repository = 
worker_container_tag = 
delete_worker_pods = True
delete_worker_pods_on_start = False
worker_pods_creation_batch_size = 1
multi_namespace_mode = False

[kubernetes_secrets]
# Kubernetes secrets configuration

[kubernetes_environment_variables]
# Kubernetes environment variables

[scheduler]
# Scheduler configuration
job_heartbeat_sec = 5
scheduler_heartbeat_sec = 5
num_runs = -1
scheduler_idle_sleep_time = 1
min_file_process_interval = 0
dag_dir_list_interval = 300
print_stats_interval = 30
pool_metrics_interval = 5.0
scheduler_health_check_threshold = 30
orphaned_tasks_check_interval = 300.0
child_process_timeout = 60
scheduler_zombie_task_threshold = 300
catchup_by_default = False
max_tis_per_query = 512
use_row_level_locking = True
max_dagruns_to_create_per_loop = 10
max_dagruns_per_loop_to_schedule = 20
schedule_after_task_execution = True
parsing_processes = 2
file_parsing_sort_mode = modified_time
allow_trigger_in_future = False

[triggerer]
# Triggerer configuration
default_capacity = 1000
auditor_capacity = 1000

[smart_sensor]
# Smart sensor configuration
use_smart_sensor = False
shard_code_upper_limit = 10000
shards = 5
sensors_enabled = NamedHivePartitionSensor

[secrets]
# Secrets backend configuration
backend = 
backend_kwargs = 

[cli]
# CLI configuration
api_client = airflow.api.client.json_client
endpoint_url = http://localhost:8080

[debug]
# Debug configuration
fail_fast = False

[api]
# API configuration
auth_backends = airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
enable_experimental_api = False
maximum_page_limit = 100
fallback_page_limit = 100

[lineage]
# Data lineage configuration
backend = 

[atlas]
# Apache Atlas integration
sasl_enabled = False
host = 
port = 21000
username = 
password = 

[hive]
# Hive configuration
default_hive_mapred_queue = 

[webserver_config]
# Additional webserver configuration

[elasticsearch]
# Elasticsearch configuration
host = 
log_id_template = {dag_id}-{task_id}-{execution_date}-{try_number}
end_of_log_mark = end_of_log
frontend = 
write_stdout = False
json_format = False
json_fields = asctime, filename, lineno, levelname, message

[elasticsearch_configs]
# Elasticsearch detailed configuration

[kerberos]
# Kerberos authentication configuration
ccache = /tmp/airflow_krb5_ccache
principal = airflow
reinit_frequency = 3600
kinit_path = kinit
keytab = airflow.keytab

[github_enterprise]
# GitHub Enterprise configuration
api_rev = v3

[admin]
# Admin configuration
hide_sensitive_variable_fields = True

[kpo]
# Kubernetes Pod Operator configuration

[metrics]
# Metrics configuration
statsd_on = True
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow
statsd_allow_list = 
statsd_block_list = 

[resource_manager]
# Resource manager configuration

[actuators]
# Actuator configuration

# ================================================================
# CUSTOM METALAYER ETL CONFIGURATION
# ================================================================

[metalayer]
# Custom configuration for MetaLayer ETL pipeline
pipeline_version = 1.0.0
default_batch_size = 10000
data_quality_threshold = 0.95
max_processing_time_minutes = 240
alert_email = data-team@company.com
bronze_retention_days = 30
silver_retention_days = 365
gold_retention_days = 2555  # 7 years
enable_data_lineage = True
enable_performance_monitoring = True
prometheus_port = 9090
grafana_port = 3000

# Connection IDs for external systems
postgres_conn_id = postgres_default
redis_conn_id = redis_default
smtp_conn_id = smtp_default

# File paths and directories
source_data_path = /opt/airflow/data
bronze_data_path = /opt/airflow/data/bronze_src
log_retention_days = 30

# Performance tuning
worker_memory_limit = 4096
max_concurrent_tasks = 16
task_timeout_minutes = 60