# =====================================================
# ETL PIPELINE - DOCKER COMPOSE CONFIGURATION
# Complete Data Engineering Stack with Monitoring
# =====================================================

version: '3.8'

# =====================================================
# NETWORK CONFIGURATION
# =====================================================
networks:
  etl_pipeline_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

# =====================================================
# VOLUME CONFIGURATION  
# =====================================================
volumes:
  # Database persistent storage
  postgres_data:
    driver: local
  
  # Airflow persistent storage
  airflow_logs:
    driver: local
  airflow_plugins:
    driver: local
    
  # Monitoring persistent storage
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
    
  # ETL data storage
  bronze_data:
    driver: local
  silver_data:
    driver: local
  gold_data:
    driver: local

# =====================================================
# ETL PIPELINE SERVICES
# =====================================================
services:
  
  # =====================================================
  # DATA WAREHOUSE - POSTGRESQL
  # =====================================================
  postgres:
    image: postgres:15-alpine
    container_name: etl_postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-etl_pipeline_2024}
      POSTGRES_DB: ${POSTGRES_DB:-airflow}
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --lc-collate=C --lc-ctype=C"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./include/sql/init_schemas_pg.sql:/docker-entrypoint-initdb.d/01_init_schemas.sql
      - ./config/postgresql.conf:/etc/postgresql/postgresql.conf
      - bronze_data:/var/lib/postgresql/bronze_data
      - silver_data:/var/lib/postgresql/silver_data
      - gold_data:/var/lib/postgresql/gold_data
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    networks:
      etl_pipeline_network:
        ipv4_address: 172.20.0.10
    restart: always
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-airflow}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'

  # =====================================================
  # REDIS - TASK QUEUE & CACHING
  # =====================================================
  redis:
    image: redis:7-alpine
    container_name: etl_redis
    ports:
      - "6379:6379"
    networks:
      etl_pipeline_network:
        ipv4_address: 172.20.0.11
    restart: always
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'

  # =====================================================
  # AIRFLOW WEBSERVER
  # =====================================================
  airflow-webserver:
    build: 
      context: .
      dockerfile: Dockerfile
      args:
        AIRFLOW_VERSION: 2.10.2
        PYTHON_VERSION: "3.11"
    container_name: etl_airflow_webserver
    environment:
      # Database Configuration
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-etl_pipeline_2024}@postgres:5432/${POSTGRES_DB:-airflow}
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-etl_pipeline_2024}@postgres:5432/${POSTGRES_DB:-airflow}
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      
      # Core Configuration
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 4
      AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: 16
      AIRFLOW__CORE__PARALLELISM: 32
      AIRFLOW__CORE__DAG_CONCURRENCY: 16
      
      # Webserver Configuration
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY}
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
      AIRFLOW__WEBSERVER__RBAC: 'true'
      AIRFLOW__WEBSERVER__AUTHENTICATE: 'true'
      
      # Security
      AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'true'
      AIRFLOW__CORE__ENABLE_XCOM_PICKLING: 'false'
      
      # Logging
      AIRFLOW__LOGGING__LOGGING_LEVEL: INFO
      AIRFLOW__LOGGING__FAB_LOGGING_LEVEL: WARN
      
      # ETL Pipeline Configuration
      ETL_ENVIRONMENT: docker
      PROMETHEUS_PUSHGATEWAY_URL: http://prometheus:9091
      
    volumes:
      - ./dags:/opt/airflow/dags:ro
      - ./include:/opt/airflow/include:ro
      - ./plugins:/opt/airflow/plugins:ro
      - ./config:/opt/airflow/config:ro
      - ./data:/opt/airflow/data:ro
      - airflow_logs:/opt/airflow/logs
      - ./scripts:/opt/airflow/scripts:ro
      - ./tests:/opt/airflow/tests:ro
    ports:
      - "${AIRFLOW_WEBSERVER_PORT:-8080}:8080"
    networks:
      etl_pipeline_network:
        ipv4_address: 172.20.0.20
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: always
    command: >
      bash -c "
        airflow db migrate &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true &&
        airflow webserver
      "
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'

  # =====================================================
  # AIRFLOW SCHEDULER  
  # =====================================================
  airflow-scheduler:
    build: 
      context: .
      dockerfile: Dockerfile
      args:
        AIRFLOW_VERSION: 2.10.2
        PYTHON_VERSION: "3.11"
    container_name: etl_airflow_scheduler
    environment:
      # Database Configuration  
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-etl_pipeline_2024}@postgres:5432/${POSTGRES_DB:-airflow}
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-etl_pipeline_2024}@postgres:5432/${POSTGRES_DB:-airflow}
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      
      # Core Configuration
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 4
      
      # Scheduler Configuration
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 30
      AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: 'false'
      AIRFLOW__SCHEDULER__MAX_DAGRUNS: 10
      
      # ETL Pipeline Configuration
      ETL_ENVIRONMENT: docker
      
    volumes:
      - ./dags:/opt/airflow/dags:ro
      - ./include:/opt/airflow/include:ro
      - ./plugins:/opt/airflow/plugins:ro
      - ./config:/opt/airflow/config:ro
      - ./data:/opt/airflow/data:ro
      - airflow_logs:/opt/airflow/logs
      - ./scripts:/opt/airflow/scripts:ro
      - ./tests:/opt/airflow/tests:ro
    networks:
      etl_pipeline_network:
        ipv4_address: 172.20.0.21
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: always
    command: airflow scheduler
    healthcheck:
      test: ["CMD", "pgrep", "-f", "airflow scheduler"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'

  # =====================================================
  # AIRFLOW WORKER
  # =====================================================
  airflow-worker:
    build: 
      context: .
      dockerfile: Dockerfile
      args:
        AIRFLOW_VERSION: 2.10.2
        PYTHON_VERSION: "3.11"
    container_name: etl_airflow_worker
    environment:
      # Database Configuration
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-etl_pipeline_2024}@postgres:5432/${POSTGRES_DB:-airflow}
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-etl_pipeline_2024}@postgres:5432/${POSTGRES_DB:-airflow}
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      
      # Core Configuration
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      
      # Worker Configuration
      AIRFLOW__CELERY__WORKER_CONCURRENCY: 8
      AIRFLOW__CELERY__WORKER_PREFETCH_MULTIPLIER: 1
      
      # ETL Pipeline Configuration
      ETL_ENVIRONMENT: docker
      
    volumes:
      - ./dags:/opt/airflow/dags:ro
      - ./include:/opt/airflow/include:ro
      - ./plugins:/opt/airflow/plugins:ro
      - ./config:/opt/airflow/config:ro
      - ./data:/opt/airflow/data
      - airflow_logs:/opt/airflow/logs
      - ./scripts:/opt/airflow/scripts:ro
      - ./tests:/opt/airflow/tests:ro
    networks:
      etl_pipeline_network:
        ipv4_address: 172.20.0.22
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: always
    command: airflow celery worker
    healthcheck:
      test: ["CMD", "pgrep", "-f", "airflow celery worker"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 3G
          cpus: '2.0'

  # =====================================================
  # MONITORING - PROMETHEUS
  # =====================================================
  prometheus:
    image: prom/prometheus:v2.45.0
    container_name: etl_prometheus
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    networks:
      etl_pipeline_network:
        ipv4_address: 172.20.0.30
    restart: always
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'

  # =====================================================
  # MONITORING - GRAFANA
  # =====================================================
  grafana:
    image: grafana/grafana:latest
    container_name: etl_grafana  
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP: 'false'
      GF_INSTALL_PLUGINS: ""
      GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS: ""
      GF_DEFAULT_APP_MODE: development
      GF_LOG_LEVEL: info
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./config/grafana/dashboards:/var/lib/grafana/dashboards:ro
    networks:
      etl_pipeline_network:
        ipv4_address: 172.20.0.31
    depends_on:
      - prometheus
    restart: always
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'

  # =====================================================
  # MONITORING - NODE EXPORTER
  # =====================================================
  node-exporter:
    image: prom/node-exporter:v1.6.0
    container_name: etl_node_exporter
    ports:
      - "${NODE_EXPORTER_PORT:-9100}:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    networks:
      etl_pipeline_network:
        ipv4_address: 172.20.0.32
    restart: always
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.2'

  # =====================================================
  # ETL METRICS EXPORTER  
  # =====================================================
  etl-metrics-exporter:
    build: 
      context: .
      dockerfile: Dockerfile
      args:
        AIRFLOW_VERSION: 2.10.2
        PYTHON_VERSION: "3.11"
    container_name: etl_metrics_exporter
    environment:
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-etl_pipeline_2024}
      POSTGRES_DB: ${POSTGRES_DB:-airflow}
      PROMETHEUS_PUSHGATEWAY_URL: http://prometheus:9091
      ETL_ENVIRONMENT: docker
    volumes:
      - ./include/monitoring:/opt/airflow/monitoring:ro
      - ./scripts:/opt/airflow/scripts:ro
    ports:
      - "${ETL_METRICS_PORT:-9200}:9200"
    networks:
      etl_pipeline_network:
        ipv4_address: 172.20.0.33
    depends_on:
      postgres:
        condition: service_healthy
      prometheus:
        condition: service_healthy
    restart: always
    command: python /opt/airflow/monitoring/metrics_exporter.py
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:9200/metrics"]
      interval: 60s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.3'

# =====================================================
# DEVELOPMENT OVERRIDES (Optional)
# =====================================================
# Uncomment for development mode
# 
# x-dev-overrides: &dev-overrides
#   volumes:
#     - ./dags:/opt/airflow/dags
#     - ./include:/opt/airflow/include  
#     - ./plugins:/opt/airflow/plugins
#   environment:
#     AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
#     AIRFLOW__LOGGING__LOGGING_LEVEL: DEBUG